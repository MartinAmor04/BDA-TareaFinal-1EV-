{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable global para almacenar las líneas extraídas del PDF\n",
    "pdf_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer las líneas del PDF\n",
    "def extract_lines_from_pdf_pymupdf(pdf_file):\n",
    "    import fitz  # PyMuPDF\n",
    "    lines = []\n",
    "    with fitz.open(pdf_file.name) as doc:\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\")  # Obtener texto como líneas completas\n",
    "            page_lines = text.splitlines()\n",
    "            lines.extend([line.strip() for line in page_lines if line.strip()])\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para realizar la búsqueda con Chroma y responder preguntas\n",
    "def search_chroma(input, pdf_file=None):\n",
    "    global pdf_lines  # Usamos la variable global pdf_lines\n",
    "    \n",
    "    try:\n",
    "        if not pdf_lines:  # Si pdf_lines está vacío, procesamos el archivo\n",
    "            pdf_lines = extract_lines_from_pdf_pymupdf(pdf_file)\n",
    "        \n",
    "        # Inicializar el modelo de embeddings y el almacenaje de vectores Chroma\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        \n",
    "        # Crear el almacén de vectores Chroma\n",
    "        vector_store = Chroma.from_texts(\n",
    "            texts=pdf_lines,\n",
    "            collection_name=\"pdf-input\",\n",
    "            embedding=embeddings,\n",
    "        )\n",
    "\n",
    "        # Crear el recuperador (retriever)\n",
    "        retriever = vector_store.as_retriever()\n",
    "\n",
    "        # Plantilla de Prompt\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        # Modelo de chat con Ollama\n",
    "        ollama_llm = \"llama3.2\"\n",
    "        model_local = ChatOllama(model=ollama_llm)\n",
    "\n",
    "        # Crear la cadena de procesamiento\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | model_local\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # Ejecutar la cadena con la consulta\n",
    "        response = chain.invoke(input)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error al procesar la consulta: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip:\n",
    " Always set type=\"messages\" in gr.ChatInterface. The default value (type=\"tuples\") is deprecated and will be removed in a future version of Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7892\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7892/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear la interfaz con Gradio\n",
    "with gr.Blocks(theme=gr.themes.Glass()) as demo:\n",
    "    gr.Markdown(\"### Chat con modelo de análisis de PDF\")\n",
    "\n",
    "    # Sección para el chat\n",
    "    chat_interface = gr.ChatInterface(\n",
    "        fn=search_chroma,\n",
    "        type=\"messages\",\n",
    "        examples=[\"¿Quién es Martin Scorsese?\", \"¿Qué relación tiene la violencia en sus películas?\", \"Háblame de Goodfellas\"],\n",
    "    )\n",
    "\n",
    "    # Espacio para subir archivos\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"### Subir un archivo para analizar:\")\n",
    "        file_uploader = gr.File(label=\"Sube tu archivo aquí\", file_types=[\".pdf\"])\n",
    "        output = gr.Textbox(label=\"Salida\", lines=10)\n",
    "        submit_button = gr.Button(\"Procesar archivo\")\n",
    "\n",
    "    # Conectar el procesamiento del archivo con el chat\n",
    "    def process_file(pdf_file):\n",
    "        global pdf_lines\n",
    "        pdf_lines = extract_lines_from_pdf_pymupdf(pdf_file)\n",
    "        return \"Archivo procesado. Puedes hacer preguntas ahora.\"\n",
    "\n",
    "    # Conectar el botón de envío a la función de procesamiento\n",
    "    submit_button.click(fn=process_file, inputs=file_uploader, outputs=output)\n",
    "\n",
    "# Lanzar la interfaz\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
