{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-MongoAtlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto implementa un sistema RAG utilizando MongoDB Atlas para almacenar y recuperar datos. La arquitectura permite aprovechar los datos almacenados en MongoDB para enriquecer las respuestas generadas por el modelo de lenguaje. Mediante un proceso de recuperación de información, el modelo accede a documentos relevantes y los utiliza para generar respuestas más precisas y contextuales a las preguntas del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo.errors import OperationFailure\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conecta a Atlas Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb+srv://username:password@mycluster.xxxxx.mongodb.net/?\")\n",
    "database = 'rag_db'\n",
    "collection = client[\"rag_db\"][\"rag_collec\"]\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME='index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga los pdfs, extraemos los textos y los dividimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron y dividieron 220 fragmentos de texto.\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\"documents/pruebaLLM.pdf\",\"documents/pruebaLLM2.pdf\", \"documents/MartinScorsese.pdf\"]  # Agrega la lista de archivos PDF\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "for document in pdf_files:\n",
    "    loader = PyPDFLoader(document) # Carga el documento\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)    # Divide el texto en chunks\n",
    "    documents = text_splitter.split_documents(data)\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "print(f\"Se cargaron y dividieron {len(all_documents)} fragmentos de texto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa los embeddings de HuggingFace y la vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    relevance_score_fn=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserta los documentos en el vector store de MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos añadidos al vector store correctamente\n"
     ]
    }
   ],
   "source": [
    "vector_store.add_documents(all_documents)\n",
    "print(\"Documentos añadidos al vector store correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intenta crear un índice de búsqueda vectorial y maneja el error si ya existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El índice 'index' ya existe.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store.create_vector_search_index(dimensions=384) # 384 por los embeddings\n",
    "except OperationFailure as e:\n",
    "    if e.code == 68:  # Código de duplicados\n",
    "        print(f\"El índice '{ATLAS_VECTOR_SEARCH_INDEX_NAME}' ya existe.\")\n",
    "    else:\n",
    "        print(f\"Error al crear el índice: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea una cadena de procesamiento de lenguaje natural utilizando un LLM local (Ollama) para responder preguntas basadas en un contexto dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt: Se define una plantilla de pregunta-respuesta que toma un contexto y una pregunta como entrada\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Local LLM: Se configura el modelo llama3.2 para generar respuestas de manera local\n",
    "ollama_llm = \"llama3.2\"\n",
    "model_local = ChatOllama(model=ollama_llm)\n",
    "\n",
    "# Chain Se construye un pipeline que pasa el contexto desde el retriever\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sí, según el contexto proporcionado, las páginas mencionadas tratan temas relacionados al amor y su complejidad. La repetición del mismo texto en diferentes documentos sugiere una insistencia o una importancia particular en la exploración de este tema. Sin embargo, no hay una descripción específica del contenido del libro que indique si trata solo del amor o si aborda otros temas relacionados.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Trata temas como el amor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sí, menciona varias referencias a Taxi Driver en el texto proporcionado. Algunos ejemplos son:\\n\\n1. La comparación del tránsito sacramental con la instancia del sacrificio que debe preceder a la salvación o purificación, citada como ejemplo del \"tránsito sacramental\" mencionado en Taxi Driver.\\n2. Una mención explícita a la película \"Taxi Driver\", donde se cita el artículo de AISTHESIS N° 37 de 2004 sobre Martin Scorsese y su interpretación de la película.\\n3. La descripción de una escena de Charlie en su habitación, encadenada con un proyector que da paso a los créditos, diseñados sobre un fondo de imágenes familiares de formato pequeño, lo que se asocia con la escena final de Taxi Driver donde Travis se apunta un tiro y el proyectoor se activa mientras Charlie se sienta en su habitación.\\n4. La mención al personaje Travis, mencionado como ejemplo de cómo Scorsese utiliza personajes que hablan mucho pero hacen poco, lo que hace referencia a la complejidad del personaje de Travis en Taxi Driver.\\n\\nEstos ejemplos demuestran cómo el texto proporcionado hace referencias y análisis a la película Taxi Driver.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Menciona algo de Taxi Driver?Pon ejemplos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusión, este sistema utiliza MongoDB Atlas para almacenar y gestionar los documentos PDF cargados, los cuales se dividen en fragmentos para ser procesados por un modelo LLM. A través de MongoDB Atlas, se garantiza una gestión eficiente de grandes volúmenes de datos. Además, el modelo LLM responde de manera coherente a las consultas, proporcionando respuestas basadas en el contenido de los documentos almacenados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
